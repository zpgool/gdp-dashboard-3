# streamlit_app.py
"""
Streamlit ì•±: ê¸°í›„ì¬í•´ë¡œ ì¸í•œ ì²­ì†Œë…„ ìˆ˜ì—… ì°¨ì§ˆ ëŒ€ì‹œë³´ë“œ
- ì—­í• : (1) ê³µê°œ ë°ì´í„° ëŒ€ì‹œë³´ë“œ (ê³µì‹ ì¶œì²˜ ì‹œë„, ì‹¤íŒ¨ ì‹œ ì˜ˆì‹œ ë°ì´í„°ë¡œ ëŒ€ì²´)
        (2) ì‚¬ìš©ì ì…ë ¥ ëŒ€ì‹œë³´ë“œ (í”„ë¡¬í”„íŠ¸ì— ì œê³µëœ í†µê³„/í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©, ì‹¤í–‰ ì¤‘ ì¶”ê°€ ì—…ë¡œë“œ X)
ìš”êµ¬ì‚¬í•­ ë°˜ì˜:
- í•œê¸€ UI, Pretendard í°íŠ¸ ì‹œë„, @st.cache_data ì‚¬ìš©, ì „ì²˜ë¦¬Â·ë¯¸ë˜ ë°ì´í„° ì œê±°, CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì œê³µ
- ë°ì´í„° í‘œì¤€í™”: date, value, group(optional)
- ì½”ë“œ ì£¼ì„ì— ì¶œì²˜(URL) ëª…ì‹œ (ì‹¤ì œ ë°ì´í„° APIê°€ ìˆì„ ê²½ìš° ì‹œë„)
- ë¡œì»¬ ìì •(Asia/Seoul ê¸°ì¤€) ì´í›„ì˜ ë°ì´í„° ì œê±°
- ì‚¬ì´ë“œë°” í•„í„° ìë™ êµ¬ì„±
ì°¸ê³ (ì˜ˆì‹œ) ì¶œì²˜ ì£¼ì„:
 - êµìœ¡ë¶€/ì§€ì—­ ë°œí‘œ(ë‰´ìŠ¤ ê¸°ì‚¬): https://www.hani.co.kr/arti/society/schooling/1208715.html
 - 2023 ì§‘ì¤‘í˜¸ìš° ë³´ë„(ë‰´ìŠ¤): https://www.newsis.com/view/?id=NISX20230716_0002378455
 - íƒœí’Â·ê¸°í›„ ë°ì´í„°(ê¸°ìƒ/ê¸°í›„): NOAA, NASA, KMA ìë£Œ(ì‹¤ì œ API ì‚¬ìš© ì‹œ êµì²´ ê¶Œì¥)
    NOAA: https://www.ncei.noaa.gov/
    NASA GPM (ê°•ìˆ˜ëŸ‰): https://gpm.nasa.gov/
    KMA (ê¸°ìƒìë£Œ): https://www.kma.go.kr/
 - ì§€ì—­ ê¸°ì‚¬ ì˜ˆ: https://www.kado.net/news/articleView.html?idxno=1198111
(ì£¼ì˜) ë³¸ ì½”ë“œëŠ” ì‹¤í–‰ ì‹œ ì™¸ë¶€ API ì ‘ì†ì„ ì‹œë„í•©ë‹ˆë‹¤. ì‹¤íŒ¨í•˜ë©´ ë‚´ë¶€ ì˜ˆì‹œ ë°ì´í„°ë¡œ ìë™ ì „í™˜í•˜ê³  ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
"""

from datetime import datetime, timedelta, timezone
import io
import sys
import os
import json

import pandas as pd
import numpy as np
import requests
import plotly.express as px
import pydeck as pdk
import streamlit as st

# ---------------------------
# ì„¤ì •: ë¡œì»¬ íƒ€ì„ì¡´(Asia/Seoul) ê¸°ì¤€ ìì • ê³„ì‚°
# ---------------------------
SEOUL_TZ = timezone(timedelta(hours=9))
def seoul_now():
    return datetime.now(SEOUL_TZ)

def seoul_midnight_today():
    now = seoul_now()
    return datetime(year=now.year, month=now.month, day=now.day, tzinfo=SEOUL_TZ)

CUTOFF = seoul_midnight_today()  # ì˜¤ëŠ˜(ë¡œì»¬ ìì •) ì´í›„ì˜ ë°ì´í„°ëŠ” ì œê±°

# ---------------------------
# Pretendard í°íŠ¸ ì‹œë„ (ì—†ìœ¼ë©´ ë¬´ì‹œ)
# ---------------------------
st.set_page_config(page_title="ê¸°í›„ìœ„ê¸°Â·ìˆ˜ì—… ì°¨ì§ˆ ëŒ€ì‹œë³´ë“œ", layout="wide")
st.markdown(
    """
    <style>
    @font-face {
        font-family: 'PretendardCustom';
        src: url('/fonts/Pretendard-Bold.ttf') format('truetype');
        font-weight: 700;
        font-style: normal;
    }
    html, body, [class*="css"]  {
        font-family: PretendardCustom, Pretendard, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

# ---------------------------
# ìœ í‹¸ë¦¬í‹°: CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ (DataFrame -> csv buffer)
# ---------------------------
def get_csv_download_link(df: pd.DataFrame, filename: str, label: str):
    csv = df.to_csv(index=False).encode('utf-8-sig')
    return st.download_button(label=label, data=csv, file_name=filename, mime='text/csv')

# ---------------------------
# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜: ê³µê°œ ë°ì´í„° ì‹œë„ (ìºì‹œ)
# ---------------------------
@st.cache_data(ttl=3600)
def load_official_datasets():
    """
    ê°€ëŠ¥í•œ ê³µì‹ ì¶œì²˜ë¥¼ ìˆœì„œëŒ€ë¡œ ì‹œë„í•´ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    - ì‹¤íŒ¨ ì‹œ ì˜ˆì‹œ(sample) ë°ì´í„°ë¡œ ëŒ€ì²´
    ë°˜í™˜: dict of DataFrames with keys: 'precip_ts', 'school_actions', 'school_damage_geo'
    """
    messages = []
    # 1) ì‹œë„: NASA / GPM ì›”ë³„ ì „ì§€êµ¬ ê°•ìˆ˜ëŸ‰(ì˜ˆì‹œ) - (ì‹¤ì œ API êµì²´ ê¶Œì¥)
    #    (ì—¬ê¸°ì„œëŠ” public API í˜¸ì¶œ ì˜ˆì‹œë¥¼ ì‹œë„í•©ë‹ˆë‹¤; ì‹¤íŒ¨í•˜ë©´ ë‚´ë¶€ ë°ì´í„° ì‚¬ìš©)
    try:
        # Dummy example: pretend to fetch monthly precipitation index from NOAA/NASA
        # NOTE: Replace below URL with real dataset API if available.
        nasa_url = "https://data.nasa.gov/resource/example_gpm_monthly_precip.json"
        r = requests.get(nasa_url, timeout=6)
        if r.status_code == 200:
            data = r.json()
            # Normalize to DataFrame (this block is placeholder; real API will differ)
            precip_df = pd.json_normalize(data)
            # Attempt best-effort parse
            if 'date' in precip_df.columns and 'precip' in precip_df.columns:
                precip_df = precip_df.rename(columns={'precip': 'value', 'date': 'date'})
                precip_df['date'] = pd.to_datetime(precip_df['date'], errors='coerce')
                precip_df = precip_df[['date', 'value']].dropna()
            else:
                raise ValueError("NASA response format unexpected")
            messages.append("NASA GPM ë°ì´í„° ë¡œë“œ ì„±ê³µ")
        else:
            raise Exception(f"NASA ìš”ì²­ ìƒíƒœ {r.status_code}")
    except Exception as e:
        messages.append("NASA/NOAA í˜¸ì¶œ ì‹¤íŒ¨: ë‚´ë¶€ ì˜ˆì‹œ ë°ì´í„°ë¡œ ëŒ€ì²´")
        # ë‚´ë¶€ ì˜ˆì‹œ: ì—°ë„ë³„(2015-2025) ê°•ìˆ˜ ì´ë²¤íŠ¸ ì§€ìˆ˜ (ìƒ˜í”Œ)
        years = pd.date_range(start="2015-01-01", end="2025-07-01", freq="YS")
        precip_df = pd.DataFrame({
            'date': years,
            'value': [np.random.uniform(20, 80) + (i*3) for i in range(len(years))]  # ì¦ê°€ ì¶”ì´ ëª¨ì‚¬
        })

    # 2) ì‹œë„: êµìœ¡ë¶€/ë‰´ìŠ¤ì—ì„œ ì§‘ê³„ëœ 'í•™êµ ìˆ˜ì—… ì¡°ì¹˜' ì‹œê³„ì—´ (ì˜ˆì‹œ)
    try:
        # No official direct CSV; attempt to query hypothetical endpoint
        edu_url = "https://api.moe.go.kr/school-actions/example_school_actions.csv"
        r2 = requests.get(edu_url, timeout=6)
        if r2.status_code == 200:
            school_actions = pd.read_csv(io.StringIO(r2.text))
            # standardize
            if 'date' not in school_actions.columns:
                # try to find date-like column
                school_actions.rename(columns={school_actions.columns[0]:'date'}, inplace=True)
            school_actions['date'] = pd.to_datetime(school_actions['date'], errors='coerce')
            # Ensure value columns exist
            if 'value' not in school_actions.columns:
                # sum of action counts into 'value'
                numeric_cols = school_actions.select_dtypes(include='number').columns
                if len(numeric_cols) > 0:
                    school_actions['value'] = school_actions[numeric_cols].sum(axis=1)
                else:
                    school_actions['value'] = 0
            school_actions = school_actions[['date','value']].dropna()
            messages.append("êµìœ¡ë¶€ í•™êµì¡°ì¹˜ ë°ì´í„° ë¡œë“œ ì„±ê³µ")
        else:
            raise Exception(f"êµìœ¡ë¶€ ìš”ì²­ ìƒíƒœ {r2.status_code}")
    except Exception as e:
        messages.append("êµìœ¡ë¶€/ë‰´ìŠ¤ API í˜¸ì¶œ ì‹¤íŒ¨: ë‚´ë¶€ ì˜ˆì‹œ ë°ì´í„°ë¡œ ëŒ€ì²´")
        # ë‚´ë¶€ ì˜ˆì‹œ: íŠ¹ì • ë‚ ì§œë³„ í•™êµ ì¡°ì¹˜ ê±´ìˆ˜ (ìƒ˜í”Œ)
        dates = pd.to_datetime([
            "2023-07-16","2023-08-10","2024-07-20","2025-07-18","2025-03-19"
        ])
        values = [24, 5, 40, 247, 12]  # ì‚¬ìš©ì ì œê³µ í†µê³„ ë°˜ì˜ ì˜ˆì‹œ
        school_actions = pd.DataFrame({'date': dates, 'value': values})

    # 3) ì‹œë„: í•™êµ í”¼í•´ ì§€ë¦¬ì •ë³´ (ìœ„ë„/ê²½ë„) - ì˜ˆì‹œ
    try:
        geo_url = "https://data.kostat.go.kr/example/school_damage_geo.geojson"
        r3 = requests.get(geo_url, timeout=6)
        if r3.status_code == 200:
            geo_json = r3.json()
            # For simplicity, create DataFrame of features with lon/lat and damage_count
            feats = geo_json.get('features', [])
            rows = []
            for f in feats:
                props = f.get('properties', {})
                geom = f.get('geometry', {})
                coords = geom.get('coordinates', [None, None])
                rows.append({
                    'lon': coords[0],
                    'lat': coords[1],
                    'value': props.get('damage_count', 1),
                    'name': props.get('name', 'í•™êµ')
                })
            school_damage_geo = pd.DataFrame(rows)
            messages.append("ì§€ë¦¬ ë°ì´í„° ë¡œë“œ ì„±ê³µ")
        else:
            raise Exception(f"ì§€ë¦¬ ë°ì´í„° ìš”ì²­ ìƒíƒœ {r3.status_code}")
    except Exception as e:
        messages.append("ì§€ë¦¬/ìœ„ì¹˜ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: ë‚´ë¶€ ì˜ˆì‹œ ë°ì´í„°ë¡œ ëŒ€ì²´")
        # ë‚´ë¶€ ì˜ˆì‹œ: ëª‡ê°œ ì§€ì—­ í¬ì¸íŠ¸
        school_damage_geo = pd.DataFrame({
            'name': ['ì²­ì£¼ ì§€ì—­í•™êµ','ì§„ì²œ í•™êµ','ì˜¥ì²œ í•™êµ','ì˜ë™ í•™êµ','ê´´ì‚° í•™êµ'],
            'lat': [36.642, 36.873, 36.305, 36.118, 36.606],
            'lon': [127.489, 127.327, 127.654, 127.761, 127.957],
            'value': [11,3,2,2,2]
        })

    # ì „ì²˜ë¦¬: í‘œì¤€í™” ë° ë¯¸ë˜ ë°ì´í„° ì œê±°
    for df in [precip_df, school_actions]:
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'], errors='coerce')
            df = df.dropna(subset=['date'])
    # remove future dates beyond today midnight (Asia/Seoul)
    def remove_future_dates(df):
        if 'date' in df.columns:
            return df[df['date'] < CUTOFF].copy()
        else:
            return df
    precip_df = remove_future_dates(precip_df)
    school_actions = remove_future_dates(school_actions)

    return {
        'precip_ts': precip_df.reset_index(drop=True),
        'school_actions': school_actions.reset_index(drop=True),
        'school_damage_geo': school_damage_geo.reset_index(drop=True),
        'messages': messages
    }

# ---------------------------
# ì‚¬ìš©ì ì…ë ¥(í”„ë¡¬í”„íŠ¸ ë‚´ í†µê³„) -> ë‚´ë¶€ë¡œì§ìœ¼ë¡œë§Œ ì‚¬ìš© (ì•± ì‹¤í–‰ ì¤‘ ì—…ë¡œë“œ ê¸ˆì§€)
# ---------------------------
@st.cache_data
def build_user_input_dataset():
    """
    ì‚¬ìš©ìê°€ ì œê³µí•œ Input ì„¹ì…˜ì˜ í†µê³„/í…ìŠ¤íŠ¸ë§Œì„ ì‚¬ìš©í•´ DataFrame ìƒì„±.
    ì•„ë˜ í•­ëª©ì€ í”„ë¡¬í”„íŠ¸ì— ì œê³µëœ í†µê³„ë“¤ì„ í‘œì¤€í™”(date, value, group)
    """
    rows = []
    # From prompt examples:
    # 2023-07 ì§‘ì¤‘í˜¸ìš°: 24ê°œ í•™êµ íœ´ì—…/ì›ê²©ìˆ˜ì—…
    rows.append({'date': pd.to_datetime("2023-07-16"), 'value': 24, 'group': 'íœ´ì—…/ì›ê²©/ì¡°ì¹˜', 'note':'2023ë…„ 7ì›” ì§‘ì¤‘í˜¸ìš°(ë‰´ìŠ¤)'} )
    # íƒœí’ ì¹´ëˆˆ 2023-08-09 ê°•ì›ë„ 5ê°œ íœ´êµ
    rows.append({'date': pd.to_datetime("2023-08-09"), 'value': 5, 'group': 'íœ´ì—…/ì›ê²©/ì¡°ì¹˜', 'note':'íƒœí’ ì¹´ëˆˆ(ê°•ì›ë„)'})
    # 2025-07 í­ìš°: 247ê°œ í•™êµ í•™ì‚¬ì¼ì • ì¡°ì • ë“±(ì„¸ë¶€ìˆ˜ì¹˜ì€ ë³„ë„)
    rows.append({'date': pd.to_datetime("2025-07-18"), 'value': 247, 'group': 'í•™ì‚¬ì¼ì • ì¡°ì •', 'note':'2025-07 ì „êµ­ í­ìš°(í•œê²¨ë ˆ ê¸°ì‚¬)'} )
    # 2025-03 í­ì„¤ ì¼ë¶€ í•™êµ íœ´ì—… (ì˜ˆ: 12)
    rows.append({'date': pd.to_datetime("2025-03-19"), 'value': 12, 'group': 'íœ´ì—…/ì›ê²©/ì¡°ì¹˜', 'note':'2025-03 ê°•ì›ë„ í­ì„¤(EBS)'} )
    # ì¶©ë¶ í˜¸ìš° í”¼í•´ 2023-07-16: í”¼í•´ í•™êµÂ·ìœ ì¹˜ì› 24ê³³ (ì§€ì—­ breakdown exists)
    # We'll also add regional breakdown entries from prompt
    rows.extend([
        {'date': pd.to_datetime("2023-07-16"), 'value': 11, 'group': 'ì²­ì£¼ í”¼í•´ í•™êµ', 'note':'ì¶©ë¶ ì²­ì£¼ í”¼í•´ í•™êµìˆ˜'},
        {'date': pd.to_datetime("2023-07-16"), 'value': 3,  'group': 'ì§„ì²œ í”¼í•´ í•™êµ', 'note':'ì¶©ë¶ ì§„ì²œ'},
        {'date': pd.to_datetime("2023-07-16"), 'value': 2,  'group': 'ì˜¥ì²œ í”¼í•´ í•™êµ', 'note':'ì¶©ë¶ ì˜¥ì²œ'},
        {'date': pd.to_datetime("2023-07-16"), 'value': 2,  'group': 'ì˜ë™ í”¼í•´ í•™êµ', 'note':'ì¶©ë¶ ì˜ë™'},
        {'date': pd.to_datetime("2023-07-16"), 'value': 2,  'group': 'ê´´ì‚° í”¼í•´ í•™êµ', 'note':'ì¶©ë¶ ê´´ì‚°'},
        {'date': pd.to_datetime("2023-07-16"), 'value': 1,  'group': 'ì œì²œ í”¼í•´ í•™êµ', 'note':'ì¶©ë¶ ì œì²œ'},
        {'date': pd.to_datetime("2023-07-16"), 'value': 1,  'group': 'ë³´ì€ í”¼í•´ í•™êµ', 'note':'ì¶©ë¶ ë³´ì€'}
    ])
    df = pd.DataFrame(rows)
    # í‘œì¤€í™”: ensure date,value,group exist
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    df = df.dropna(subset=['date'])
    df['value'] = pd.to_numeric(df['value'], errors='coerce').fillna(0).astype(int)
    # remove future dates beyond cutoff (per rules)
    df = df[df['date'] < CUTOFF].reset_index(drop=True)
    return df

# ---------------------------
# ë©”ì¸ UI
# ---------------------------
st.title("ê¸°í›„ìœ„ê¸°(í­ìš°Â·íƒœí’ ë“±) â†’ ì²­ì†Œë…„ ìˆ˜ì—… ì°¨ì§ˆ ëŒ€ì‹œë³´ë“œ")
st.caption("ê³µê°œ ë°ì´í„°(ê³µì‹ ì¶œì²˜ ì‹œë„)ì™€ ì‚¬ìš©ì ì…ë ¥(í”„ë¡¬í”„íŠ¸ í†µê³„)ì„ ê°ê° ì‹œê°í™”í•©ë‹ˆë‹¤. ëª¨ë“  ë¼ë²¨ì€ í•œêµ­ì–´ì…ë‹ˆë‹¤.")

# Load official datasets (with caching)
with st.spinner("ê³µê°œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘... (ê³µì‹ ì¶œì²˜ ì‹œë„)"):
    data_bundle = load_official_datasets()
precip = data_bundle['precip_ts']
school_actions = data_bundle['school_actions']
school_geo = data_bundle['school_damage_geo']
for msg in data_bundle.get('messages', []):
    st.info(msg)

# íƒ­ êµ¬ì„±: ê³µê°œ ë°ì´í„° / ì‚¬ìš©ì ì…ë ¥
tab1, tab2 = st.tabs(["ğŸ“¡ ê³µê°œ ë°ì´í„° ëŒ€ì‹œë³´ë“œ", "ğŸ“ ì‚¬ìš©ì ì…ë ¥ ëŒ€ì‹œë³´ë“œ"])

# ---------------------------
# íƒ­1: ê³µê°œ ë°ì´í„°
# ---------------------------
with tab1:
    st.header("ê³µê°œ ë°ì´í„° ê¸°ë°˜ ë¶„ì„ (ê³µì‹ ì¶œì²˜ ì‹œë„)")
    st.markdown("**ì¶œì²˜(ì˜ˆì‹œ)**: êµìœ¡ë¶€/ë‰´ìŠ¤ ê¸°ì‚¬(í•œê²¨ë ˆ, ë‰´ì‹œìŠ¤), ê¸°ìƒÂ·ê¸°í›„(êµ­ì œ: NOAA/NASA, êµ­ë‚´: KMA). ì½”ë“œ ë‚´ë¶€ ì£¼ì„ì— ìƒì„¸ ì¶œì²˜ í‘œê¸°.")
    col1, col2 = st.columns([2,1])
    with col1:
        st.subheader("ì—°ë„ë³„(ë˜ëŠ” ì‹œê³„ì—´) ê°•ìˆ˜/ê¸°ìƒ ì§€í‘œ (ì˜ˆì‹œ)")
        if precip is None or precip.empty:
            st.warning("ê°•ìˆ˜ ì‹œê³„ì—´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‚´ë¶€ ì˜ˆì‹œ ë°ì´í„°ê°€ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            st.write("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
            st.dataframe(precip.head(10))
        # ê¸°ë³¸ í”Œë¡¯: êº¾ì€ì„  (ì—°ë„/ì‹œê³„ì—´)
        if not precip.empty:
            # ìë™ ì‚¬ì´ë“œë°” ì˜µì…˜: ê¸°ê°„ í•„í„° / ìŠ¤ë¬´ë”©(ì´ë™í‰ê· )
            min_date = precip['date'].min().date()
            max_date = precip['date'].max().date()
            with st.expander("ì‹œê³„ì—´ í•„í„° ì˜µì…˜"):
                start = st.date_input("ì‹œì‘ì¼", value=min_date, min_value=min_date, max_value=max_date, key="precip_start")
                end = st.date_input("ì¢…ë£Œì¼", value=max_date, min_value=min_date, max_value=max_date, key="precip_end")
                window = st.slider("ì´ë™í‰ê·  ìœˆë„ìš°(ê¸°ê°„)", min_value=1, max_value=12, value=1, key="precip_ma")
            mask = (precip['date'].dt.date >= start) & (precip['date'].dt.date <= end)
            df_plot = precip.loc[mask].copy()
            df_plot = df_plot.sort_values('date')
            if window > 1:
                df_plot['value_smooth'] = df_plot['value'].rolling(window=window, min_periods=1).mean()
                fig = px.line(df_plot, x='date', y='value_smooth', labels={'value_smooth':'ê°’(ì´ë™í‰ê· )','date':'ë‚ ì§œ'}, title="ê°•ìˆ˜ëŸ‰ ì§€í‘œ(ì´ë™í‰ê· )")
            else:
                fig = px.line(df_plot, x='date', y='value', labels={'value':'ê°’','date':'ë‚ ì§œ'}, title="ê°•ìˆ˜ëŸ‰ ì§€í‘œ")
            fig.update_layout(xaxis_title="ë‚ ì§œ", yaxis_title="ê°•ìˆ˜ ì§€í‘œ (ì„ì˜ ë‹¨ìœ„)")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("ê°•ìˆ˜ ì‹œê³„ì—´ ë°ì´í„°ê°€ ì—†ì–´ ì˜ˆì‹œ ê·¸ë˜í”„ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.")
            # ì˜ˆì‹œ ê·¸ë˜í”„: ëœë¤ ìƒì„± (ì´ë¯¸ ë‚´ë¶€ ì˜ˆì‹œ ë°ì´í„° ìˆìŒ)
            sample = precip.copy()
            if not sample.empty:
                fig = px.line(sample, x='date', y='value', labels={'value':'ê°’','date':'ë‚ ì§œ'}, title="ê°•ìˆ˜ëŸ‰ ì§€í‘œ (ì˜ˆì‹œ)")
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.write("ë°ì´í„° ì—†ìŒ")

    with col2:
        st.subheader("í•™êµ ì¡°ì¹˜(íœ´ì—…/ì›ê²©/ë‹¨ì¶• ë“±) ì‹œê³„ì—´")
        st.write("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
        st.dataframe(school_actions.head(10))
        if not school_actions.empty:
            # ìë™ ì‚¬ì´ë“œë°” ì˜µì…˜: ì§‘ê³„ë‹¨ìœ„(ë‚ /ì›”/ì—°)
            agg_opt = st.selectbox("ì§‘ê³„ ë‹¨ìœ„ ì„ íƒ", options=['ì¼ë³„','ì›”ë³„','ì—°ë³„'], index=1, key='agg_school')
            df_sa = school_actions.copy()
            if agg_opt == 'ì›”ë³„':
                df_sa['period'] = df_sa['date'].dt.to_period('M').dt.to_timestamp()
            elif agg_opt == 'ì—°ë³„':
                df_sa['period'] = df_sa['date'].dt.to_period('Y').dt.to_timestamp()
            else:
                df_sa['period'] = df_sa['date'].dt.floor('D')
            df_agg = df_sa.groupby('period', as_index=False)['value'].sum().sort_values('period')
            fig2 = px.bar(df_agg, x='period', y='value', labels={'period':'ê¸°ê°„','value':'ì¡°ì¹˜ ê±´ìˆ˜'}, title="í•™êµ ì¡°ì¹˜ ê±´ìˆ˜ ì¶”ì´")
            st.plotly_chart(fig2, use_container_width=True)
            # CSV ë‹¤ìš´ë¡œë“œ
            get_csv_download_link(df_agg, "public_school_actions_agg.csv", "í•™êµ ì¡°ì¹˜ ì§‘ê³„ CSV ë‹¤ìš´ë¡œë“œ")
        else:
            st.info("ê³µê°œëœ í•™êµ ì¡°ì¹˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‚´ë¶€ ì˜ˆì‹œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    st.subheader("í•™êµ í”¼í•´ ìœ„ì¹˜ ì§€ë„ (ì˜ˆì‹œÂ·ìƒ˜í”Œ)")
    if school_geo is not None and not school_geo.empty:
        st.write("ì§€ë„ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
        st.dataframe(school_geo.head(10))
        # pydeck ì§€ë„
        if {'lat','lon'}.issubset(set(school_geo.columns)):
            midpoint = (float(school_geo['lat'].mean()), float(school_geo['lon'].mean()))
            st.pydeck_chart(pdk.Deck(
                map_style='mapbox://styles/mapbox/light-v9',
                initial_view_state=pdk.ViewState(
                    latitude=midpoint[0],
                    longitude=midpoint[1],
                    zoom=7,
                    pitch=0,
                ),
                layers=[
                    pdk.Layer(
                        "ScatterplotLayer",
                        data=school_geo,
                        get_position='[lon, lat]',
                        get_radius='value * 200',
                        radius_min_pixels=5,
                        radius_max_pixels=100,
                        get_fill_color='[255, 100, 100, 140]',
                        pickable=True,
                    )
                ],
                tooltip={"text": "{name}\ní”¼í•´ê±´ìˆ˜: {value}"}
            ))
            get_csv_download_link(school_geo, "public_school_damage_geo.csv", "í•™êµ í”¼í•´ ìœ„ì¹˜ CSV ë‹¤ìš´ë¡œë“œ")
        else:
            st.warning("í•™êµ ì§€ë¦¬ë°ì´í„°ì— lat/lon ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    else:
        st.info("í•™êµ í”¼í•´ ìœ„ì¹˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("---")
    st.markdown("**[ì•Œë¦¼]**: ìœ„ ê³µê°œ ë°ì´í„°ëŠ” ì•± ì‹¤í–‰ ì‹œ ì™¸ë¶€ APIë¥¼ ì‹œë„í•´ ê°€ì ¸ì˜¤ë©°, API ì‹¤íŒ¨ ì‹œ ë‚´ë¶€ ì˜ˆì‹œ ë°ì´í„°ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤. ì½”ë“œ ì£¼ì„ì˜ ì¶œì²˜(ë‰´ìŠ¤/ê¸°ìƒ ë°ì´í„°)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.")

# ---------------------------
# íƒ­2: ì‚¬ìš©ì ì…ë ¥ (í”„ë¡¬í”„íŠ¸ì˜ í†µê³„ë§Œ ì‚¬ìš©)
# ---------------------------
with tab2:
    st.header("ì‚¬ìš©ì ì…ë ¥ ëŒ€ì‹œë³´ë“œ (í”„ë¡¬í”„íŠ¸ ë‚´ ì œê³µ ë°ì´í„°ë§Œ ì‚¬ìš©)")
    st.markdown("ì•± ì‹¤í–‰ ì¤‘ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìš”êµ¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì•„ë˜ ì‹œê°í™”ëŠ” í”„ë¡¬í”„íŠ¸ì— ì œê³µëœ í†µê³„(ì—°ë„ë³„/ì‚¬ê±´ë³„ í•™êµ ì¡°ì¹˜ ë° ì§€ì—­ë³„ í”¼í•´)ë¥¼ ì‚¬ìš©í•´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    user_df = build_user_input_dataset()
    st.subheader("ë°ì´í„° í‘œì¤€í™” ë¯¸ë¦¬ë³´ê¸°")
    st.dataframe(user_df)

    # ìë™ ì‚¬ì´ë“œë°” ì˜µì…˜: í•„í„°(ê·¸ë£¹), ê¸°ê°„
    st.subheader("ëŒ€ì‹œë³´ë“œ ì»¨íŠ¸ë¡¤")
    groups = sorted(user_df['group'].unique().tolist())
    sel_groups = st.multiselect("ê·¸ë£¹ ì„ íƒ (ê¸°ë³¸: ì „ì²´)", options=groups, default=groups, key='user_groups')
    min_date = user_df['date'].min().date()
    max_date = user_df['date'].max().date()
    start = st.date_input("ì‹œì‘ì¼", value=min_date, min_value=min_date, max_value=max_date, key='user_start')
    end = st.date_input("ì¢…ë£Œì¼", value=max_date, min_value=min_date, max_value=max_date, key='user_end')
    df_filtered = user_df[(user_df['date'].dt.date >= start) & (user_df['date'].dt.date <= end)]
    if sel_groups:
        df_filtered = df_filtered[df_filtered['group'].isin(sel_groups)]

    st.subheader("ì‹œê³„ì—´: í•™êµ ì¡°ì¹˜ / ì‚¬ê±´ë³„ ê±´ìˆ˜")
    # If multiple dates per year, use line/area; else bar
    df_ts = df_filtered.groupby('date', as_index=False)['value'].sum().sort_values('date')
    if df_ts.empty:
        st.info("ì„ íƒëœ ì¡°ê±´ì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # Choose plot type by number of distinct dates
        if df_ts['date'].nunique() > 6:
            fig_u = px.area(df_ts, x='date', y='value', labels={'date':'ë‚ ì§œ','value':'ê±´ìˆ˜'}, title="í•™êµ ì¡°ì¹˜(ì‹œê³„ì—´)")
        else:
            fig_u = px.bar(df_ts, x='date', y='value', labels={'date':'ë‚ ì§œ','value':'ê±´ìˆ˜'}, title="í•™êµ ì¡°ì¹˜(ì‚¬ê±´ë³„)")
        st.plotly_chart(fig_u, use_container_width=True)

    st.subheader("ê·¸ë£¹ë³„ ë¶„í¬ (ì›ê·¸ë˜í”„ / ë§‰ëŒ€)")
    df_group = df_filtered.groupby('group', as_index=False)['value'].sum().sort_values('value', ascending=False)
    if not df_group.empty:
        fig_pie = px.pie(df_group, names='group', values='value', title="ê·¸ë£¹ë³„ í•©ê³„(ë¹„ìœ¨)")
        fig_bar = px.bar(df_group, x='group', y='value', labels={'group':'ê·¸ë£¹','value':'í•©ê³„'}, title="ê·¸ë£¹ë³„ í•©ê³„(ë§‰ëŒ€)")
        st.plotly_chart(fig_pie, use_container_width=True)
        st.plotly_chart(fig_bar, use_container_width=True)
        get_csv_download_link(df_filtered, "user_input_filtered.csv", "ì‚¬ìš©ì ì…ë ¥(í•„í„°ëœ) CSV ë‹¤ìš´ë¡œë“œ")
    else:
        st.info("ê·¸ë£¹ë³„ ë°ì´í„° ì—†ìŒ")

    st.subheader("ì§€ì—­ë³„ í”¼í•´(í”„ë¡¬í”„íŠ¸ ì§€ì—­ë‚´ì—­ ê¸°ë°˜)")
    # Build small map from user_df region groups that contain 'í”¼í•´' or known region names
    region_df = user_df[user_df['group'].str.contains('í”¼í•´|ì²­ì£¼|ì§„ì²œ|ì˜¥ì²œ|ì˜ë™|ê´´ì‚°|ì œì²œ|ë³´ì€', na=False)].copy()
    # assign approximate coords (from earlier internal sample in load_official_datasets); if missing, skip map
    coord_map = {
        'ì²­ì£¼ í”¼í•´ í•™êµ': (36.642,127.489),
        'ì§„ì²œ í”¼í•´ í•™êµ': (36.873,127.327),
        'ì˜¥ì²œ í”¼í•´ í•™êµ': (36.305,127.654),
        'ì˜ë™ í”¼í•´ í•™êµ': (36.118,127.761),
        'ê´´ì‚° í”¼í•´ í•™êµ': (36.606,127.957),
        'ì œì²œ í”¼í•´ í•™êµ': (37.128,128.194),
        'ë³´ì€ í”¼í•´ í•™êµ': (36.487,127.721)
    }
    if not region_df.empty:
        rows = []
        for _, r in region_df.iterrows():
            grp = r['group']
            if grp in coord_map:
                lat, lon = coord_map[grp]
                rows.append({'name':grp, 'lat':lat, 'lon':lon, 'value':int(r['value'])})
        if rows:
            reg_df = pd.DataFrame(rows)
            st.dataframe(reg_df)
            midpoint = (float(reg_df['lat'].mean()), float(reg_df['lon'].mean()))
            st.pydeck_chart(pdk.Deck(
                map_style='mapbox://styles/mapbox/light-v9',
                initial_view_state=pdk.ViewState(
                    latitude=midpoint[0],
                    longitude=midpoint[1],
                    zoom=8,
                    pitch=0,
                ),
                layers=[
                    pdk.Layer(
                        "ScatterplotLayer",
                        data=reg_df,
                        get_position='[lon, lat]',
                        get_radius='value * 300',
                        radius_min_pixels=5,
                        radius_max_pixels=200,
                        get_fill_color='[50, 130, 200, 160]',
                        pickable=True,
                    )
                ],
                tooltip={"text": "{name}\ní”¼í•´ í•™êµìˆ˜: {value}"}
            ))
            get_csv_download_link(reg_df, "user_region_damage.csv", "ì§€ì—­ í”¼í•´ CSV ë‹¤ìš´ë¡œë“œ")
        else:
            st.info("ì§€ì—­ ì¢Œí‘œ ë§¤í•‘ ê°€ëŠ¥í•œ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("í”„ë¡¬í”„íŠ¸ì— ì§€ì—­ í”¼í•´ ë°ì´í„°ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

    st.markdown("---")
    st.subheader("ìš”ì•½ ë° ê¶Œì¥ ì‹œê°í™”")
    st.markdown("""
    - ì œê³µëœ í†µê³„(2023-07: 24ê°œ, 2023-08 íƒœí’ ì¹´ëˆˆ: 5ê°œ, 2025-07: 247ê°œ ë“±)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ì‹œê³„ì—´(ì—°/ì›”ë³„ ì¶”ì´), ê·¸ë£¹ë³„ ë¹„ìœ¨(ì›/ë§‰ëŒ€), ì§€ì—­ ë¶„í¬ ì§€ë„** ë“±ì„ ìë™ ìƒì„±í–ˆìŠµë‹ˆë‹¤.
    - ì¥ê¸°ì ìœ¼ë¡œëŠ” ê¸°ìƒ(ê°•ìˆ˜) ì‹œê³„ì—´ê³¼ í•™êµ ì¡°ì¹˜ ì‹œê³„ì—´ì„ ë³‘ë ¬ë¡œ ë³´ì—¬ì£¼ì–´ ìƒê´€/ì—°ê´€ì„ íƒìƒ‰í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
    """)

# ---------------------------
# í•˜ë‹¨: ì¶”ê°€ ì•ˆë‚´(ê°„ë‹¨)
# ---------------------------
st.sidebar.header("ì•± ì •ë³´")
st.sidebar.write("ì´ ì•±ì€ Streamlit + GitHub Codespaces í™˜ê²½ì—ì„œ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n- ì™¸ë¶€ API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ë‚´ë¶€ ì˜ˆì‹œ ë°ì´í„°ë¡œ ìë™ ëŒ€ì²´ë©ë‹ˆë‹¤.\n- Pretendard í°íŠ¸ëŠ” /fonts/Pretendard-Bold.ttf ê²½ë¡œì—ì„œ ì‹œë„í•˜ë©°, ì—†ìœ¼ë©´ ì‹œìŠ¤í…œ í°íŠ¸ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.\n- ì½”ë“œ ìˆ˜ì • ì‹œ ì£¼ì„ì— í‘œê¸°ëœ ì¶œì²˜(URL)ë¥¼ ì‹¤ì œ ë°ì´í„°ì…‹ ì—”ë“œí¬ì¸íŠ¸ë¡œ êµì²´í•˜ì„¸ìš”.")

st.sidebar.header("ê°œë°œì ë…¸íŠ¸")
st.sidebar.write("""
- Kaggle ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ kaggle API í† í°ì„ Codespacesì— ì—…ë¡œë“œí•˜ê³  í™˜ê²½ë³€ìˆ˜ KAGGLE_CONFIG_DIRë¥¼ ì„¤ì •í•œ ë’¤ kaggle ëª…ë ¹ì–´ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.
- ì˜ˆì‹œ:
  1) kaggle.jsonì„ ~/.kaggle/kaggle.jsonìœ¼ë¡œ ì—…ë¡œë“œ
  2) !kaggle datasets download -d <dataset> --unzip
- êµìœ¡ë¶€/ê¸°ìƒì²­ ë“± ê³µì‹ ë°ì´í„°ì˜ ê³µê°œ ì—”ë“œí¬ì¸íŠ¸ë¡œ êµì²´í•˜ë©´ ë”ìš± ì •í™•í•œ ëŒ€ì‹œë³´ë“œê°€ ë©ë‹ˆë‹¤.
""")


